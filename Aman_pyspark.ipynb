{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Aman_pyspark.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPhFkgDj60jVaO9Seb8tv7p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amankumar114/1st/blob/master/Aman_pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHZ7w3khO_ID",
        "outputId": "73fcbd8e-d219-4a2a-d3e1-bba926e60aff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "300\n"
          ]
        }
      ],
      "source": [
        "x = 100\n",
        "y = 200\n",
        "print(x+y)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Hello all, good afternoon,we are learning Spark and we are not happy\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQEdWOLjPc4e",
        "outputId": "0e58a897-83fe-475b-934c-cf396a01d0dc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello all, good afternoon,we are learning Spark and we are not happy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzDREjBiPrOh",
        "outputId": "393c258f-d736-479e-e7cf-63bc28611c29"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 46 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 61.9 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=3e51566f13ee1e8abaadb2a7f51ce2aaccf1622cf5af7f4ef76f527e4e46829b\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/8e/1b/f73a52650d2e5f337708d9f6a1750d451a7349a867f928b885\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "6VEvQSO9QAQf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #SparkSession -entrypoint to spark - creates Session for the user \n",
        "\n",
        "from pyspark.sql import SparkSession \n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "id": "g-uuHtQLUNUK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lCcEVIWxfU1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample RDD creation - RDD is resilient distributed dataset - its basic building block in Spark\n",
        "from pyspark.sql import SparkSession \n",
        "spark = SparkSession.builder.appName(\"Pyspark create RDD example\").config(\"spark.some.config.option\",\"some-value\").getOrCreate()\n",
        "Student = spark.createDataFrame([\n",
        "('10001','Anuj','70%','B.Tech(cs)'),\n",
        "('10002','Madhu','70%','B.Tech(EC)'),\n",
        "('10003','Manita','70%','B.Tech(TE)'),\n",
        "('10004','Sudiksha','70%','B.Tech(BT)'),\n",
        "('10005','Shashank','70%','B.Tech(cs)'),\n",
        "('10006','Prashant','70%','B.Tech(cs)'),\n",
        "('10007','Vanshika','70%','B.Tech(EC)')],\n",
        "['Roll No','Name','Percentage','Department']\n",
        ")\n",
        "\n",
        "Student.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHC4ZHnXW4a-",
        "outputId": "f3136a6a-7438-474a-f70c-5091e423b89a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------+----------+----------+\n",
            "|Roll No|    Name|Percentage|Department|\n",
            "+-------+--------+----------+----------+\n",
            "|  10001|    Anuj|       70%|B.Tech(cs)|\n",
            "|  10002|   Madhu|       70%|B.Tech(EC)|\n",
            "|  10003|  Manita|       70%|B.Tech(TE)|\n",
            "|  10004|Sudiksha|       70%|B.Tech(BT)|\n",
            "|  10005|Shashank|       70%|B.Tech(cs)|\n",
            "|  10006|Prashant|       70%|B.Tech(cs)|\n",
            "|  10007|Vanshika|       70%|B.Tech(EC)|\n",
            "+-------+--------+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sample RDD creation - using parallelize method\n",
        "\n",
        "from pyspark.sql import SparkSession \n",
        "spark = SparkSession.builder.appName(\"Pyspark create RDD example\").config(\"spark.some.config.option\",\"some-value\").getOrCreate()\n",
        "df = spark.sparkContext.parallelize ([\n",
        "(12,20,35,'a b c'),\n",
        "(41,34,64,'d e f'),\n",
        "(70,85,68,'g e f')]).toDF(['col1','col2','col3','col4'])                                      \n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-6d1rF1jIMH",
        "outputId": "eb307265-f969-45b0-edb7-afb7d6bcfde7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+----+-----+\n",
            "|col1|col2|col3| col4|\n",
            "+----+----+----+-----+\n",
            "|  12|  20|  35|a b c|\n",
            "|  41|  34|  64|d e f|\n",
            "|  70|  85|  68|g e f|\n",
            "+----+----+----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPE7aa5XfV3C",
        "outputId": "72b35cf2-6cfa-4f60-d284-e23c932f70b1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.3.0)\n",
            "Requirement already satisfied: py4j==0.10.9.5 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading sample data from dataframe\n",
        "\n",
        "import pandas as pd \n",
        "df = pd.DataFrame.from_dict(\n",
        "    { 'Name':['Nik','Nik','Jane','Jane'],\n",
        "     'Year' : [2020,2021,2022,2019] ,\n",
        "     'Sales' : [1000,2300,2500,3000],\n",
        "\n",
        "} )\n",
        "\n",
        "print(df)\n",
        "\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYoykVHJgOoP",
        "outputId": "bc9184c0-5232-4003-9d99-f5378b47c711"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Name  Year  Sales\n",
            "0   Nik  2020   1000\n",
            "1   Nik  2021   2300\n",
            "2  Jane  2022   2500\n",
            "3  Jane  2019   3000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataframe creation from a list of rows :\n",
        "\n",
        "from datetime import datetime , date\n",
        "import pandas as pd \n",
        "from pyspark.sql import Row\n",
        "df = spark.createDataFrame([\n",
        "          Row(a=1, b=2, c='string1', d=date(2022, 1,1), e= datetime(2022,1,1, 12,0)),  \n",
        "          Row(a=2, b=3, c='string2', d=date(2022, 2,1), e= datetime(2022,1,2, 12,0)),            \n",
        "          Row(a=3, b=4, c='string3', d=date(2022, 3,1), e= datetime(2022,1,3, 12,0)),                      \n",
        "])\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuziWaEiox36",
        "outputId": "03804734-dd36-47db-b0eb-19a965311685"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+-------+----------+-------------------+\n",
            "|  a|  b|      c|         d|                  e|\n",
            "+---+---+-------+----------+-------------------+\n",
            "|  1|  2|string1|2022-01-01|2022-01-01 12:00:00|\n",
            "|  2|  3|string2|2022-02-01|2022-01-02 12:00:00|\n",
            "|  3|  4|string3|2022-03-01|2022-01-03 12:00:00|\n",
            "+---+---+-------+----------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0N2iw09qA8T",
        "outputId": "b7af9112-b6e4-4286-e57e-51a82ea3842f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+-------+----------+-------------------+\n",
            "|  a|  b|      c|         d|                  e|\n",
            "+---+---+-------+----------+-------------------+\n",
            "|  1|  2|string1|2022-01-01|2022-01-01 12:00:00|\n",
            "+---+---+-------+----------+-------------------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEKgyvvhqA_O",
        "outputId": "cbfe92c7-0b0e-4a3b-ed95-d9f36806e8bd"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+-------+----------+-------------------+\n",
            "|  a|  b|      c|         d|                  e|\n",
            "+---+---+-------+----------+-------------------+\n",
            "|  1|  2|string1|2022-01-01|2022-01-01 12:00:00|\n",
            "|  2|  3|string2|2022-02-01|2022-01-02 12:00:00|\n",
            "|  3|  4|string3|2022-03-01|2022-01-03 12:00:00|\n",
            "+---+---+-------+----------+-------------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}